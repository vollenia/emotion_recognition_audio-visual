# emotion_recognition_audio-visual

The goal of this project is to train and evaluate a model that would be able to automatically recognize one of 4 emotion classes (Happy, Angry, Sad, Neutral) from a speech signal and a video stream by combining the two modalities via model-level fusion.
